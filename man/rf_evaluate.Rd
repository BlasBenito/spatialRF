% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rf_evaluate.R
\name{rf_evaluate}
\alias{rf_evaluate}
\title{Evaluates random forest models with spatial cross-validation}
\usage{
rf_evaluate(
  model = NULL,
  xy = NULL,
  repetitions = 30,
  training.fraction = 0.75,
  distance.step = NULL,
  swap.spatial.folds = FALSE,
  seed = 1,
  verbose = TRUE,
  n.cores = parallel::detectCores() - 1,
  cluster = NULL
)
}
\arguments{
\item{model}{(required; model) Fitted with \code{\link[=rf]{rf()}}, \code{\link[=rf_repeat]{rf_repeat()}}, or \code{\link[=rf_spatial]{rf_spatial()}}.}

\item{xy}{(required; data frame or matrix) Must have two columns named "x" and "y" containing the geographic coordinates of the sampling locations. Default: \code{NULL}}

\item{repetitions}{(optional; integer) Number of spatial folds to use during cross-validation. Must be lower than the total number of rows available in the training data. Default: \code{30}}

\item{training.fraction}{(optional; numeric) Proportion of records to be used as training set during spatial cross-validation. Default: \code{0.75}}

\item{distance.step}{(optional; numeric) Numeric vector of length one or two. Distance step used during the growth of the buffer containing the training cases. Must be in the same units as the coordinates in \code{xy}. When only one distance is provided, the same growth is applied to the x and y axes. If two distances are provided, the first one is applied to the x axis, and the second one to the y. When \code{NULL}, it uses 1/1000th of the range of each axis as distance. The smaller this number is, the easier is to achieve an accurate \code{training.fraction}, but the slower the algorithm becomes. Default: \code{NULL}}

\item{swap.spatial.folds}{(optional; logical) If true, the cases inside the rectangular buffer are used as testing set instead of training. This can help in edge cases when the data distribution is highly irregular. Default: \code{FALSE}}

\item{seed}{(optional; integer) Random seed to facilitate reproduciblity. If set to a given number, the results of the function are always the same. Default: \code{1}.}

\item{verbose}{(optional; logical) If \code{TRUE}, messages and plots generated during the execution of the function are displayed, Default: \code{TRUE}}

\item{n.cores}{(optional; integer) Number of cores used by \code{\link[ranger]{ranger}} for parallel execution (used as value for the argument \code{num.threads} in \code{ranger()}). Default: \code{parallel::detectCores() - 1}}

\item{cluster}{(optional; cluster object) Cluster definition generated with \code{parallel::makeCluster()} or \code{\link[=start_cluster]{start_cluster()}}. Overrides \code{n.cores}. Faster than using \code{n.cores} for smaller models. This function does not stop a cluster, please remember to shut it down with \code{parallel::stopCluster(cl = cluster_name)} or \code{\link[=stop_cluster]{stop_cluster()}} at the end of your pipeline. Default: \code{NULL}}
}
\value{
A model of the class "rf_evaluate" with a new slot named "evaluation". Additionally, spatial cross-validation performance scores are added to the "performance" slot of the model.

Objects written to \code{model$evaluation}:
\itemize{
\item \code{spatial.folds}: list of lists with the training and testing spatial folds used during the evaluation. Each element of the list corresponds to a row in the \code{per.fold} data frame.
\item \code{training.fraction}: Value of the argument \code{training.fraction}.
\item \code{per.fold}: Data frame with the cross-validation results per spatial fold. It contains the numeric id of each fold, it's central coordinates, the number of training and testing cases, and the training and testing performance metrics.
\item \code{aggregated}: Aggregated version of \code{per.fold} with median, median absolute deviation, quartile 1, quartile 3, mean, standard error, standard deviation, minimum, and maximum performance scores across spatial folds.
\item \code{roc_full}, \code{roc_oob}, and \code{roc_scv}: Produced only when the response is binary (values one and zero). Lists of data frames with the in-bag (_full suffix), out-of-bag (_oob), and spatial cross-validation (_scv) confusion matrices, sensitivity and specificity across all spatial folds.
}

Objects written to \code{model$performance}:
\itemize{
\item
}
}
\description{
Evaluates the performance of random forest on unseen data over independent spatial folds.
}
\details{
The evaluation algorithm works as follows:

Generation of contiguous training folds:

\itemize{
\item 1. The function \code{\link[=thinning_til_n]{thinning_til_n()}} finds a set of coordinates from \code{xy} of size \code{repetitions}. These pairs of coordinates are as separated as possible, and will be used as "training fold centers".
\item 2. From each training fold-center, a rectangular buffer is grown from the training fold center one step at a time until it encloses a number of records in \code{data} as close as possible to the \code{training.fraction}. The step wise growth of this buffer is controlled by the argument \code{distance.step}.
\item 3. The indices of all records in \code{data} within the buffer are written to the "training" vector of the nested list.
\item 4. The indices of the remaining records are written to the "testing" list.
\item 5. In each repetition, a model is fitted with the training cases.
\item 6. The predictions of the model in 5. are compared with the values of the response in the testing records to compute model performance metrics.
}

If the response variable is continuous, the metrics used are "rsquared", "rmse", and "nrmse" (normalized rmse).

If the response is binary (zeros and ones), the metrics used are "auc" (area under the ROC curve), "rsquared" (point-biserial correlation), and "roc" (components of the confusion matrix for different prediction thresholds).

In any case, these metrics are identified by the suffix "_scv" (from "spatial cross-validation").

The medians of the cross-validation performance metrics are stored in the "performance" slot of the model, while the complete results are stored in the "evaluation" slot.
}
\examples{
if(interactive()){

#loading example data
data(
  ecoregions_df,
  ecoregions_distance_matrix,
  ecoregions_numeric_predictors,
  ecoregions_continuous_response
  )

#fitting random forest model
rf.model <- rf(
  data = ecoregions_df,
  dependent.variable.name = ecoregions_continuous_response,
  predictor.variable.names = ecoregions_numeric_predictors,
  distance.matrix = ecoregions_distance_matrix,
  distance.thresholds = 0,
  n.cores = 1,
  verbose = FALSE
)

#evaluation with spatial cross-validation
rf.model <- rf_evaluate(
  model = rf.model,
  xy = ecoregions_df[, c("x", "y")],
  n.cores = 1
)

#checking evaluation results
plot_evaluation(rf.model)
print_evaluation(rf.model)
x <- get_evaluation_aggregated(rf.model)

}
}
